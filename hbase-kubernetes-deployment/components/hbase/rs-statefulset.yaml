# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: regionserver
  labels:
    role: regionserver
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      role: regionserver
  serviceName: hadoop
  template:
    metadata:
      labels:
        role: regionserver
    spec:
      serviceAccountName: hadoop
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                role: regionserver
            topologyKey: kubernetes.io/hostname
      containers:
      - image: hbase
        name: hbase
        command:
          - /bin/bash
          - -c
          - |-
            # Shell context so we can pull in the environment variables set in the container and
            # via the env and envFrom.
            #
            # See https://stackoverflow.com/questions/57885828/netty-cannot-access-class-jdk-internal-misc-unsafe
            HBASE_CLASSPATH=/etc/hadoop \
            HBASE_OPTS=" \
              -XX:MaxRAMPercentage=${JVM_HEAP_PERCENTAGE_OF_RESOURCE_LIMIT} \
              -XX:InitialRAMPercentage=${JVM_HEAP_PERCENTAGE_OF_RESOURCE_LIMIT} \
              -javaagent:${JMX_PROMETHEUS_JAR}=7000:/tmp/scratch/jmxexporter.yaml \
              -Djava.security.properties=/tmp/scratch/java.security \
              -Djava.library.path=/var/lib/hadoop-native-lib/native \
                --add-opens java.base/jdk.internal.misc=ALL-UNNAMED \
              -Djava.util.logging.config.file=${HBASE_CONF_DIR}/logging.properties \
              -Dio.netty.tryReflectionSetAccessible=true \
              -Xlog:gc:${HBASE_LOG_DIR}/gc.log:time,uptime:filecount=10,filesize=100M" \
            hbase-daemon.sh --config /etc/hbase foreground_start regionserver \
              ${FAILOVER_PROXY_PROVIDER} # &> ${HBASE_LOG_DIR}/regionserver.$(date -u +"%Y-%m-%dT%H%M%SZ").out
        # For now, just fetch local /jmx
        # Says kubelet only exposes failures, not success: https://stackoverflow.com/questions/34455040/kubernetes-liveness-probe-logging
        # TODO: Use https://hbase.apache.org/devapidocs/org/apache/hadoop/hbase/HealthChecker.html
        livenessProbe:
          httpGet:
            path: /jmx?qry=java.lang:type=OperatingSystem
            port: 16030
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /jmx?qry=java.lang:type=OperatingSystem
            port: 16030
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /jmx?qry=java.lang:type=OperatingSystem
            port: 16030
          initialDelaySeconds: 10
          failureThreshold: 30
          periodSeconds: 10
        resources:
          requests:
            memory: "2Gi"
            cpu: "0.2"
          limits:
            memory: "3Gi"
            cpu: "1.0"
        envFrom:
        - configMapRef:
            name: environment
        - configMapRef:
            name: zookeeper-quorum
        - configMapRef:
            name: jaegertracing
            optional: true
        - configMapRef:
            name: xraytracing
            optional: true
        env:
# not honored by opentelemetry-java/v1.0.1
#          - name: OTEL_EXPERIMENTAL_SDK_ENABLED
#            # flip this switch to disable tracing
#            value: "true"
# not honored by opentelemetry-java/v1.0.1
#          - name: OTEL_SERVICE_NAME
#            value: "hbase-regionserver"
#          - name: HBASE_TRACE_OPTS
#            value: "true"
#          - name: OTEL_RESOURCE_ATTRIBUTES
#            value: "service.name=hbase-regionserver"
#          - name: HBASE_SHELL_OPTS
#            # it appears that system properties override environment variables, so naming the shell
#            # can via system property will override the master name provided by environment variable
#            value: "-Dotel.service.name=hbase-shell"
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - containerPort: 16030
          name: http
        - containerPort: 16020
          name: rpc
        volumeMounts:
          - name: hbase-configuration
            mountPath: /etc/hbase
            readOnly: true
          - name: hbck2-configuration
            mountPath: /etc/hbck2
            readOnly: true
          - mountPath: /etc/hadoop
            name: hadoop-configuration
          - mountPath: /var/log/hbase
            name: hbase-logs
          - mountPath: /tmp/scratch
            name: scratch
          - mountPath: /tmp/scripts
            name: scripts
          - mountPath: /var/lib/hadoop-native-lib
            name: hadoop-native-lib
      initContainers:
      - image: hbase
        name: bootstrapper
        imagePullPolicy: IfNotPresent
        command:
        - /bin/bash
        - -c
        - |-
          set -ex
          # Wait on a single master to be up...
          /tmp/scripts/jmxping.sh master ${HADOOP_SERVICE} 1
          cp /tmp/global-files/* /tmp/scratch/
        securityContext:
          # Run bootstrapper as root so can set ${USER} owner on data volume
          allowPrivilegeEscalation: false
          runAsUser: 0
        resources:
          requests:
            cpu: '0.2'
            memory: 256Mi
          limits:
            cpu: '0.5'
            memory: 512Mi
        envFrom:
        - configMapRef:
            name: environment
        volumeMounts:
        - mountPath: /tmp/scripts
          name: scripts
        # Scratch dir is a location where init containers place items for later use
        # by  the main containers when they run.
        - mountPath: /tmp/scratch
          name: scratch
        - mountPath: /tmp/global-files
          name: global-files
      - image: hadoop
        name: copy-hadoop-native-lib
        command:
        # Runs as the image/hdfs user.
        - /bin/bash
        - -c
        - |-
          set -xe
          if [ -d ~/hadoop/lib/native ]; then
            cp -r ~/hadoop/lib/native /var/lib/hadoop-native-lib
          else
            echo "Native dir not found at ~/hadoop/lib/native"
          fi
        resources:
          requests:
            cpu: '0.2'
            memory: 256Mi
          limits:
            cpu: '0.5'
            memory: 512Mi
        envFrom:
        - configMapRef:
            name: environment
        volumeMounts:
        - mountPath: /var/lib/hadoop-native-lib
          name: hadoop-native-lib
      restartPolicy: Always
      volumes:
        - name: hbase-configuration
          configMap:
            name: hbase-configuration
        - name: hbck2-configuration
          configMap:
            name: hbck2-configuration
        - configMap:
            name: hadoop-configuration
          name: hadoop-configuration
        - configMap:
            name: global-files
          name: global-files
        - emptyDir: {}
          name: hbase-logs
        - configMap:
            name: scripts
            defaultMode: 0555
          name: scripts
        # Scratch dir is a location where init containers place items for later use
        # by  the main containers when they run.
        - emptyDir: {}
          name: scratch
        - emptyDir: {}
          name: hadoop-native-lib
