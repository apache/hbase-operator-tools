# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: datanode
spec:
  podManagementPolicy: Parallel
  replicas: 1
  selector:
    matchLabels:
      role: datanode
  serviceName: hadoop
  template:
    metadata:
      labels:
        role: datanode
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                role: datanode
            topologyKey: kubernetes.io/hostname
      containers:
      - image: hadoop
        name: datanode
        command:
          - /bin/bash
          - -c
          - |-
            # Shell context so we can pull in the environment variables set in the container and
            # via the env and envFrom.
            # See https://stackoverflow.com/questions/57885828/netty-cannot-access-class-jdk-internal-misc-unsafe
            HADOOP_LOGFILE="hdfs-${HOSTNAME}.log" \
            HDFS_DATANODE_OPTS=" \
              -XX:MaxRAMPercentage=${JVM_HEAP_PERCENTAGE_OF_RESOURCE_LIMIT} \
              -XX:InitialRAMPercentage=${JVM_HEAP_PERCENTAGE_OF_RESOURCE_LIMIT} \
              -Djava.security.properties=/tmp/scratch/java.security \
              -javaagent:${JMX_PROMETHEUS_JAR}=8000:/tmp/scratch/jmxexporter.yaml \
              -Djava.library.path=${HADOOP_HOME}/lib/native \
              --add-opens java.base/jdk.internal.misc=ALL-UNNAMED \
              -Dio.netty.tryReflectionSetAccessible=true \
              -Xlog:gc:/var/log/hadoop/gc.log:time,uptime:filecount=10,filesize=100M" \
            hdfs datanode
        # For now, just fetch local /jmx
        # Says kubelet only exposes failures, not success: https://stackoverflow.com/questions/34455040/kubernetes-liveness-probe-logging
        # Do better. Check this DN successfully registered w/ NN. TODO.
        livenessProbe:
          httpGet:
            path: /jmx?qry=java.lang:type=OperatingSystem
            # 9865 if HTTPS
            port: 9864
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /jmx?qry=java.lang:type=OperatingSystem
            # 9865 if HTTPS
            port: 9864
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /jmx?qry=java.lang:type=OperatingSystem
            # 9865 if HTTPS
            port: 9864
          initialDelaySeconds: 10
          failureThreshold: 30
          periodSeconds: 10
        resources:
          requests:
            cpu: '0.2'
            memory: 1Gi
          limits:
            cpu: '1.0'
            memory: 1.5Gi
        envFrom:
        - configMapRef:
            name: environment
        env:
        # The 'node' this container is running on, not hdfs namenode.
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - name: http
          containerPort: 9864
        - name: https
          containerPort: 9865
        - name: data
          containerPort: 9866
        - name: ipc
          containerPort: 9867
        - name: jmx
          containerPort: 9864
        volumeMounts:
        - mountPath: /etc/hadoop
          name: hadoop-configuration
        - mountPath: /var/log/hadoop
          name: hadoop-logs
        - mountPath: /tmp/scratch
          name: scratch
        - mountPath: /tmp/scripts
          name: scripts
        - mountPath: /data00
          name: data00
      initContainers:
      - image: hadoop
        name: bootstrapper
        command:
        - /bin/bash
        - -c
        - |-
          set -xe
          mkdir -p ${HADOOP_LOG_DIR} || echo $?
          chown -R ${USER} ${HADOOP_LOG_DIR}
          # If format-hdfs configmap present, format.
          ! /tmp/scripts/exists_configmap.sh format-hdfs || (
            for dir in $( echo "${DATANODE_DATA_DIR}" | tr ',' '\n')
            do
              rm -rf ${dir}
            done
          )
          for dir in $( echo "${DATANODE_DATA_DIR}" | tr ',' '\n')
          do
            mkdir -p ${dir} || :
            chown -R ${USER} ${dir}
          done
          df -h
          cp /tmp/global-files/* /tmp/scratch/
          # Wait for the nns to come up.
          /tmp/scripts/jmxping.sh namenode ${HADOOP_SERVICE}
        securityContext:
          # Run bootstrapper as root so can set ${USER} owner on data volume
          allowPrivilegeEscalation: false
          runAsUser: 0
        resources:
          requests:
            cpu: '0.2'
            memory: 256Mi
          limits:
            cpu: '0.5'
            memory: 512Mi
        envFrom:
        - configMapRef:
            name: environment
        env:
        # Used by scripts that run during bootstrap
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - mountPath: /data00
          name: data00
        - mountPath: /tmp/scripts
          name: scripts
        # Scratch dir is a location where init containers place items for later use
        # by  the main containers when they run.
        - mountPath: /tmp/scratch
          name: scratch
        - mountPath: /tmp/global-files
          name: global-files
      serviceAccountName: hadoop
      volumes:
      - configMap:
          name: hadoop-configuration
        name: hadoop-configuration
      - configMap:
          name: scripts
          defaultMode: 0555
        name: scripts
      - configMap:
          name: global-files
        name: global-files
      - emptyDir: {}
        name: hadoop-logs
      # Scratch dir is a location where init containers place items for later use
      # by  the main containers when they run.
      - emptyDir: {}
        name: scratch
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: data00
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
