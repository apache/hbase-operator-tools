# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
---
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: journalnode
spec:
  minAvailable: 1
  selector:
    matchLabels:
      role: journalnode
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: journalnode
spec:
  podManagementPolicy: Parallel
  replicas: 5
  selector:
    matchLabels:
      role: journalnode
  serviceName: hadoop
  template:
    metadata:
      labels:
        role: journalnode
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - podAffinityTerm:
              labelSelector:
                matchLabels:
                  role: journalnode
              topologyKey: kubernetes.io/hostname
            weight: 30
      containers:
      - image: hadoop
        name: journalnode
        imagePullPolicy: IfNotPresent
        command:
          - /bin/bash
          - -c
          - |-
            # Shell context so we can pull in the environment variables set in the container and
            # via the env and envFrom.
            # See https://stackoverflow.com/questions/57885828/netty-cannot-access-class-jdk-internal-misc-unsafe
            HADOOP_LOGFILE="hdfs-${HOSTNAME}.log" \
            HDFS_JOURNALNODE_OPTS=" \
              -XX:MaxRAMPercentage=${JVM_HEAP_PERCENTAGE_OF_RESOURCE_LIMIT} \
              -XX:InitialRAMPercentage=${JVM_HEAP_PERCENTAGE_OF_RESOURCE_LIMIT} \
              -javaagent:${JMX_PROMETHEUS_JAR}=8000:/tmp/scratch/jmxexporter.yaml \
              -Djava.security.properties=/tmp/scratch/java.security \
              -Djava.library.path=${HADOOP_HOME}/lib/native \
              --add-opens java.base/jdk.internal.misc=ALL-UNNAMED \
              -Dio.netty.tryReflectionSetAccessible=true \
              -Xlog:gc:/var/log/hadoop/gc.log:time,uptime:filecount=10,filesize=100M" \
            hdfs journalnode
        # For now, just fetch local /jmx
        # Says kubelet only exposes failures, not success: https://stackoverflow.com/questions/34455040/kubernetes-liveness-probe-logging
        # Do better. Check this DN successfully registered w/ NN. TODO.
        livenessProbe:
          httpGet:
            # Could look to see if jn is 'formatted': JournalsStatus" : "{\"hadoop\":{\"Formatted\":\"true\"}}""
            path: /jmx?qry=Hadoop:service=JournalNode,name=JournalNodeInfo
            port: 8480
          initialDelaySeconds: 10
          periodSeconds: 10
          failureThreshold: 3
        startupProbe:
          exec:
            command:
              - /bin/bash
              - -c
              - |-
                log="./startProbe.log"
                probeResult=0
                if [ ! -z "$( ls -A ${JOURNALNODE_DATA_DIR} )" ]; then
                  # The data dir is not empty. The NN has formatted. The JN has been restarted. In this case, I wanted
                  # to check the logs for ‘Preallocated 1048576 bytes at the end of the edit log (offset 0)‘.
                  # Instances of this message are present when journalnode has successfully opened a journal.
                  # (There may be other indicators: e.g. 'Updating lastPromisedEpoch from 2 to 3 for client /10.244.3.64 ; journal id: hadoop')
                  # Before this point, it will 'JournalOutOfSyncException: Can't write, no segment open ; journal id: hadoop'
                  # which the NN gets and considers a failure; too many replicas in this state and
                  # NN exits. This happens across a rolling restart of JNs. I'd rather hold up the startup
                  # until the journal is open before letting startup proceed. Unfortunately, I have to let
                  # journalnode go so it shows up and joins the cluster... and finds out what Journal to open.
                  # Because of this I can't hold here in startup phase (nor in readiness phase... same issue happens
                  # when readiness holds the JN offline preventing it from joining cluster waiting on 'Preallocated' in
                  # logs).  Instead, wait as long as possible, wait till after webserver is up and it is ready listening
                  # before letting startup proceed.
                  echo "`date` ${JOURNALNODE_DATA_DIR} is NOT empty" >> $log
                  grep -q -e "IPC Server listener on .*: starting" ${HADOOP_LOG_DIR}/hdfs*.log
                  probeResult=$?
                fi
                # Just fall thought... we are initializing hdfs.
                echo "`date` probeResult=$probeResult" >> $log
                exit $probeResult
          initialDelaySeconds: 1
          failureThreshold: 30
          periodSeconds: 10
        resources:
          limits:
            cpu: "1.0"
            memory: 1.5Gi
          requests:
            cpu: "0.5"
            memory: 1Gi
        envFrom:
        - configMapRef:
            name: environment
        env:
        # The 'node' this container is running on, not hdfs namenode.
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        ports:
        - containerPort: 8480
          name: http
        - containerPort: 8481
          name: https
        - containerPort: 8480
          name: jmx
        - containerPort: 8485
          name: rpc
        volumeMounts:
        - mountPath: /etc/hadoop
          name: hadoop-configuration
        - mountPath: /var/log/hadoop
          name: hadoop-logs
        - mountPath: /tmp/scratch
          name: scratch
        - mountPath: /tmp/scripts
          name: scripts
        - mountPath: /data00
          name: data00
      initContainers:
      - image: hadoop
        name: bootstrapper
        imagePullPolicy: IfNotPresent
        command:
        # Check if we need to cleanup our data dir. Then load up
        # the certificate, key, and keystores into the /tmp/scratch directory
        # for use when main container launches.
        - /bin/bash
        - -c
        - |-
          set -ex
          mkdir -p ${HADOOP_LOG_DIR} || echo $?
          chown -R ${USER} ${HADOOP_LOG_DIR}
          # If format-hdfs configmap present, format.
          if /tmp/scripts/exists_configmap.sh format-hdfs; then
            rm -rf ${JOURNALNODE_DATA_DIR}
          fi
          mkdir -p ${JOURNALNODE_DATA_DIR}
          chown -R ${USER} ${JOURNALNODE_DATA_DIR}
          cp /tmp/global-files/* /tmp/scratch/
        securityContext:
          # Run bootstrapper as root so can set ${USER} owner on data volume
          allowPrivilegeEscalation: false
          runAsUser: 0
        resources:
          requests:
            cpu: '0.2'
            memory: 256Mi
          limits:
            cpu: '0.5'
            memory: 512Mi
        envFrom:
        - configMapRef:
            name: environment
        env:
        # Used by scripts that run during bootstrap
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        volumeMounts:
        - mountPath: /data00
          name: data00
        - mountPath: /tmp/scripts
          name: scripts
        # Scratch dir is a location where init containers place items for later use
        # by  the main containers when they run.
        - mountPath: /tmp/scratch
          name: scratch
        - mountPath: /tmp/global-files
          name: global-files
      serviceAccountName: hadoop
      volumes:
      - configMap:
          name: hadoop-configuration
        name: hadoop-configuration
      - configMap:
          name: global-files
        name: global-files
      - emptyDir: {}
        name: hadoop-logs
      - configMap:
          name: scripts
          defaultMode: 0555
        name: scripts
      # Scratch dir is a location where init containers place items for later use
      # by  the main containers when they run.
      - emptyDir: {}
        name: scratch
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: data00
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 2Gi
